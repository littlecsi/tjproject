testing_iris$Species2[testing_iris$Species == 'setosa'] <- 1
testing_iris$Species2[testing_iris$Species == 'versicolor'] <- 2
testing_iris$Species2[testing_iris$Species == 'virginica'] <- 3
testing_iris$Species <- NULL # 기존 칼럼 제거
head(testing_iris)
# 데이터 정규화 : 연산시 0과 1사이의 값으로 변환하여 속도 향상을 기대할 수 있다.
# 학습 데이터와 검정 데이터의 칼럼을 대상으로 0과 1사이의 값으로 정규화 한다.
normalization <- function(x){
return (( x - min(x)) / (max(x) - min(x)))
}
training_nor <- as.data.frame(lapply(training_iris, normalization))
summary(training_nor) # 0과 1사이의 칼럼과 확인
testing_nor <- as.data.frame(lapply(testing_iris, normalization))
summary(testing_nor)
# 인공 신경망 모델 생성 : 은닉 노드 1개
# 형식) neuralnet(formula, data, hidden)
model_net =
neuralnet(Species2 ~ Sepal.Length+Sepal.Width+Petal.Length+ Petal.Width,
data=training_nor, hidden = 1)
class(model_net)
plot(model_net) # 인공 신경망 시각화.png
# 은닉층 노드 수 : 1개
# 입력층에서 4개의 입력 변수에 대한 가중치와 경계값(bias) 가중치가 은닉층으로 전달되고,
# 은닉층의 결과에 대한 가중치와 경계값 가중치가 출력층으로 전달되는 과정을 볼 수 있다.
# help(compute)
# 분류 모델 성능 평가
# (1) 모델의 예측치 생성 : compute()함수 이용
model_result <- compute(model_net, testing_nor[c(1:4)])
model_result
model_result$net.result # 분류 예측값 보기
# 분류 정확도 구하는 요령
# 모델의 예측치(model_result$net.result)와 검정 데이터의 y 변수를
# 이용하여 피어슨의 상관 계수를 구하면 된다.로
# 즉 예측된 꽃의 종과 실제 관측치(testing_nor) 사이의 상관 관계를 측정한다.
cor(model_result$net.result, testing_nor$Species2)
# 0.9591815561
# 분류 모델 성능 향상 : 은닉층 노드 2개 지정, backprop 속성 적용
model_net2 =
neuralnet(Species2 ~ Sepal.Length+Sepal.Width+Petal.Length+ Petal.Width,
data=training_nor, hidden = 2,
algorithm="backprop", learningrate=0.01 )
# (2) 분류모델 예측치 생성과 평가
model_result2 <- compute(model_net2, testing_nor[c(1:4)])
cor(model_result2$net.result, testing_nor$Species2)
# 0.9589408419
# 분류 모델의 성능을 향상 시키기 위한 방법
# 은닉층의 노드(hidden)를 증가시키기
# 역전파 알고리즘 등의 적용
# learningrate 조정하기
training_nor
normalization <- function(x){
return (( x - min(x)) / (max(x) - min(x)))
}
training_iris
trData
trData$currCmt
lapply(trData$currCmt, normalization)
str(trData)
lapply(trData$currCmt, normalization)
sapply(trData$currCmt, normalization)
apply(trData$currCmt, normalization)
tapply(trData$currCmt, normalization)
tapply(trData$currCmt, fun=normalization)
tapply(trData$currCmt, c(1,2), fun=normalization)
apply(trData$currCmt, fun=normalization)
apply(trData$currCmt, FUN=normalization)
apply(trData$currCmt, c(1,2), FUN=normalization)
length(trData$currCmt)
sapply(trData$currCmt, normalization)
scale(trData$currCmt)
mode(scale(trData$currCmt))
class(scale(trData$currCmt))
as.vector(scale(trData$currCmt))
NewsDataN <- data.frame()
for(i in c(1:11)) {
NewsDataN[,i] <- as.vector(scale(NewsData$currCmt))
}
NewsDataN[,12] <- NewsData[,12]
colnames(NewsDataN) <- colnames(NewsData[,12])
NewsdataN
NewsDataN <- data.frame()
NewsDataN
sapply(NewsData$currCmt, scale)
NewsData$currCmt
NewsDataN <- data.frame(as.vector(scale(NewsData[,1])))
for(i in c(2:11)) {
vec <- as.vector(scale(NewsData[,i]))
t_df <- data.frame(vec)
NewsDataN <- cbind(NewsDataN, t_df)
}
NewsDataN
colnames(NewsdataN)
colnames(NewsDataN)
NewsDataN$section <- NewsData[,12]
colnames(NewsDataN) <- colnames(NewsData[,12])
colnames(NewsDataN)
NewsDataN
str(NewsDataN)
colnames(NewsDataN) <- colnames(NewsData)
colnames(NewsDataN)
NewsDataN
Elen <- nrow(subset(NewsData, section == "Economy"))
Ilen <- nrow(subset(NewsData, section == "IT"))
Llen <- nrow(subset(NewsData, section == "Life_Cult"))
Plen <- nrow(subset(NewsData, section == "Politics"))
Slen <- nrow(subset(NewsData, section == "Society"))
Wlen <- nrow(subset(NewsData, section == "World"))
Ei <- sample(c(1:Elen), 0.75*Elen); Ii <- sample(c(1:Ilen), 0.75*Ilen); Li <- sample(c(1:Llen), 0.75*Llen);
Pi <- sample(c(1:Plen), 0.75*Plen); Si <- sample(c(1:Slen), 0.75*Slen); Wi <- sample(c(1:Wlen), 0.75*Wlen);
idx <- c(
Ei,
Ii + Elen,
Li + Elen + Ilen,
Pi + Elen + Ilen + Llen,
Si + Elen + Ilen + Llen + Plen,
Wi + Elen + Ilen + Llen + Plen + Slen
)
NewsDataN <- data.frame(as.vector(scale(NewsData[,1])))
for(i in c(2:11)) {
vec <- as.vector(scale(NewsData[,i]))
t_df <- data.frame(vec)
NewsDataN <- cbind(NewsDataN, t_df)
}
NewsDataN$section <- NewsData[,12]
colnames(NewsDataN) <- colnames(NewsData)
trData <- NewsDataN[idx,]
teData <- NewsDataN[-idx,]
nnFormula <- section ~ .
nnModel2 <- neuralnet(formula=nnFormula, data=trData, hidden=10)
trData
str(trData)
nnModel2 <- neuralnet(formula=nnFormula, data=trData, hidden=1)
nnModel2 <- neuralnet(formula=nnFormula, data=trData[c(1:500),], hidden=1)
trData[c(1:500),]
trData[c(1:100),]
nnModel2 <- neuralnet(formula=section~., data=trData[c(1:100),], hidden=1)
trData
training_nor
training_nor
trData
colnames(trData)
ncol(trData)
nrow(trData)
nnModel2 <- neuralnet(formula=nnFormula, data=trData, hidden=10)
####################################################################################################
# Library
library(openxlsx)
library(dplyr)
library(devtools)
library(nnet)
library(neuralnet)
# 시각화 R 코드 함수 다운로드
source_url('https://gist.githubusercontent.com/fawda123/7471137/raw/466c1474d0a505ff044412703516c34f1a4684a5/nnet_plot_update.r')
####################################################################################################
# Variable
pathOne <- "D:/GitHub/tjproject/resources/"
sections <- c("econ", "IT", "life_cult", "politics", "soc", "world")
####################################################################################################
# Function
getData <- function(path, n) {
df <- data.frame()
for(i in c(1:n)) {
dfOne <- read.xlsx(path, sheet=i, colNames=T, rowNames=F)
df <- rbind(df, dfOne)
}
return(df)
}
getAllSectionData <- function(section) {
path1 <- paste(pathOne, section, "/2018_comment_data_", section, ".xlsx", sep="")
dfOne <- getData(path1, 2)
path2 <- paste(pathOne, section, "/2019_comment_data_", section, ".xlsx", sep="")
dfTwo <- getData(path2, 10)
path3 <- paste(pathOne, section, "/2018_view_data_", section, ".xlsx", sep="")
dfThree <- getData(path3, 2)
path4 <- paste(pathOne, section, "/2019_view_data_", section, ".xlsx", sep="")
dfFour <- getData(path4, 10)
df <- rbind(dfOne[-5], dfTwo[-5], dfThree[-5], dfFour[-5])
return(df)
}
getcolumns <- function(data, columns) {
len <- length(columns)
if(len > 1) {
df <- data %>% select(columns[1])
for(i in c(2:len)) {
col <- columns[i]
df <- cbind(df, data[col])
}
} else { df <- data %>% select(columns) }
return(df)
}
calcAcc <- function(compTable) {
len <- nrow(compTable)
total <- 0
for(l in c(1:len)) { total <- total + compTable[l,l] }
accuracy <- round((total / sum(compTable)) * 100, 2)
return(accuracy)
}
normalization <- function(x){
return (( x - min(x)) / (max(x) - min(x)))
}
####################################################################################################
# Main
# Collect Data
Edf <- getAllSectionData(sections[1])
Idf <- getAllSectionData(sections[2])
Ldf <- getAllSectionData(sections[3])
Pdf <- getAllSectionData(sections[4])
Sdf <- getAllSectionData(sections[5])
Wdf <- getAllSectionData(sections[6])
# Get needed columns
reqCol <- c("currCmt", "deleted", "brokenPolicy", "maleRatio", "femaleRatio", "X10", "X20", "X30", "X40", "X50", "X60")
Edata <- getcolumns(Edf, reqCol)
Idata <- getcolumns(Idf, reqCol)
Ldata <- getcolumns(Ldf, reqCol)
Pdata <- getcolumns(Pdf, reqCol)
Sdata <- getcolumns(Sdf, reqCol)
Wdata <- getcolumns(Wdf, reqCol)
# Add its own section into a column
Edata$section <- "Economy"; Idata$section <- "IT"; Ldata$section <- "Life_Cult";
Pdata$section <- "Politics"; Sdata$section <- "Society"; Wdata$section <- "World";
# Collect into one massive column
NewsData <- rbind(Edata, Idata, Ldata, Pdata, Sdata, Wdata)
colnames(NewsData)
# Remove NAs
NewsData <- na.omit(NewsData)
# Change all Independent Variables to numeric
NewsData$currCmt <- as.numeric(NewsData$currCmt)
NewsData$deleted <- as.numeric(NewsData$deleted)
NewsData$brokenPolicy <- as.numeric(NewsData$brokenPolicy)
# One-Hot Encoding
section.ind <- class.ind(NewsData$section)
NewsDataE <- cbind(NewsData, section.ind)
colnames(NewsDataE)
#  [1] "currCmt"      "deleted"      "brokenPolicy" "maleRatio"    "femaleRatio"
#  [6] "X10"          "X20"          "X30"          "X40"          "X50"
# [11] "X60"          "section"      "Economy"      "IT"           "Life_Cult"
# [16] "Politics"     "Society"      "World"
# nnet package
Elen <- nrow(subset(NewsDataE, section == "Economy"))
Ilen <- nrow(subset(NewsDataE, section == "IT"))
Llen <- nrow(subset(NewsDataE, section == "Life_Cult"))
Plen <- nrow(subset(NewsDataE, section == "Politics"))
Slen <- nrow(subset(NewsDataE, section == "Society"))
Wlen <- nrow(subset(NewsDataE, section == "World"))
Ei <- sample(c(1:Elen), 0.8*Elen); Ii <- sample(c(1:Ilen), 0.8*Ilen); Li <- sample(c(1:Llen), 0.8*Llen);
Pi <- sample(c(1:Plen), 0.8*Plen); Si <- sample(c(1:Slen), 0.8*Slen); Wi <- sample(c(1:Wlen), 0.8*Wlen);
idx <- c(
Ei,
Ii + Elen,
Li + Elen + Ilen,
Pi + Elen + Ilen + Llen,
Si + Elen + Ilen + Llen + Plen,
Wi + Elen + Ilen + Llen + Plen + Slen
)
trData <- NewsDataE[idx,]
teData <- NewsDataE[-idx,]
trX <- trData[,c(1:11)]; trY <- trData[,c(13:18)];
teX <- teData[,c(1:11)]
nnModel <- nnet(x=trX, y=trY, size=15, softmax=T)
plot.nnet(nnModel)
nnPrediction <- predict(nnModel, teX, type="class")
predTable <- table(nnPrediction, teData$section)
predAccuracy <- cat(calcAcc(predTable), "%\n")
# neuralnet package
Elen <- nrow(subset(NewsData, section == "Economy"))
Ilen <- nrow(subset(NewsData, section == "IT"))
Llen <- nrow(subset(NewsData, section == "Life_Cult"))
Plen <- nrow(subset(NewsData, section == "Politics"))
Slen <- nrow(subset(NewsData, section == "Society"))
Wlen <- nrow(subset(NewsData, section == "World"))
Ei <- sample(c(1:Elen), 0.75*Elen); Ii <- sample(c(1:Ilen), 0.75*Ilen); Li <- sample(c(1:Llen), 0.75*Llen);
Pi <- sample(c(1:Plen), 0.75*Plen); Si <- sample(c(1:Slen), 0.75*Slen); Wi <- sample(c(1:Wlen), 0.75*Wlen);
idx <- c(
Ei,
Ii + Elen,
Li + Elen + Ilen,
Pi + Elen + Ilen + Llen,
Si + Elen + Ilen + Llen + Plen,
Wi + Elen + Ilen + Llen + Plen + Slen
)
NewsDataN <- data.frame(as.vector(scale(NewsData[,1])))
for(i in c(2:11)) {
vec <- as.vector(scale(NewsData[,i]))
t_df <- data.frame(vec)
NewsDataN <- cbind(NewsDataN, t_df)
}
NewsDataN$section <- NewsData[,12]
colnames(NewsDataN) <- colnames(NewsData)
trData <- NewsDataN[idx,]
teData <- NewsDataN[-idx,]
nnFormula <- section ~ .
nnModel2 <- neuralnet(formula=nnFormula, data=trData, hidden=10)
Elen <- nrow(subset(NewsDataE, section == "Economy"))
Ilen <- nrow(subset(NewsDataE, section == "IT"))
Llen <- nrow(subset(NewsDataE, section == "Life_Cult"))
Plen <- nrow(subset(NewsDataE, section == "Politics"))
Slen <- nrow(subset(NewsDataE, section == "Society"))
Wlen <- nrow(subset(NewsDataE, section == "World"))
Ei <- sample(c(1:Elen), 0.8*Elen); Ii <- sample(c(1:Ilen), 0.8*Ilen); Li <- sample(c(1:Llen), 0.8*Llen);
Pi <- sample(c(1:Plen), 0.8*Plen); Si <- sample(c(1:Slen), 0.8*Slen); Wi <- sample(c(1:Wlen), 0.8*Wlen);
idx <- c(
Ei,
Ii + Elen,
Li + Elen + Ilen,
Pi + Elen + Ilen + Llen,
Si + Elen + Ilen + Llen + Plen,
Wi + Elen + Ilen + Llen + Plen + Slen
)
trData <- NewsDataE[idx,]
teData <- NewsDataE[-idx,]
trX <- trData[,c(1:11)]; trY <- trData[,c(13:18)];
teX <- teData[,c(1:11)]
nnModel <- nnet(x=trX, y=trY, size=15, softmax=T)
plot.nnet(nnModel)
nnPrediction <- predict(nnModel, teX, type="class")
predTable <- table(nnPrediction, teData$section)
predAccuracy <- cat(calcAcc(predTable), "%\n")
# neuralnet package
Elen <- nrow(subset(NewsData, section == "Economy"))
Ilen <- nrow(subset(NewsData, section == "IT"))
Llen <- nrow(subset(NewsData, section == "Life_Cult"))
Plen <- nrow(subset(NewsData, section == "Politics"))
Slen <- nrow(subset(NewsData, section == "Society"))
Wlen <- nrow(subset(NewsData, section == "World"))
Ei <- sample(c(1:Elen), 0.75*Elen); Ii <- sample(c(1:Ilen), 0.75*Ilen); Li <- sample(c(1:Llen), 0.75*Llen);
Pi <- sample(c(1:Plen), 0.75*Plen); Si <- sample(c(1:Slen), 0.75*Slen); Wi <- sample(c(1:Wlen), 0.75*Wlen);
idx <- c(
Ei,
Ii + Elen,
Li + Elen + Ilen,
Pi + Elen + Ilen + Llen,
Si + Elen + Ilen + Llen + Plen,
Wi + Elen + Ilen + Llen + Plen + Slen
)
NewsDataN <- data.frame(as.vector(scale(NewsData[,1])))
for(i in c(2:11)) {
vec <- as.vector(scale(NewsData[,i]))
t_df <- data.frame(vec)
NewsDataN <- cbind(NewsDataN, t_df)
}
NewsDataN$section <- NewsData[,12]
colnames(NewsDataN) <- colnames(NewsData)
trData <- NewsDataN[idx,]
teData <- NewsDataN[-idx,]
nnFormula <- section ~ .
nnModel2 <- neuralnet(formula=nnFormula, data=trData, hidden=10)
plot(nnModel2)
nnModel2Result <- compute(nnModel2, teData[,c(1:11)])
nnPrediction2 <- nnModel2Result$net.result
nndf <- data.frame(predict=nnPrediction2, real=teData["section"])
x <- seq(1:length(nnPrediction2))
plot(x=x, y=nnPrediction2, type="n", xlab="", ylab="value")
points(x, nnPrediction2, pch=4, col="red")
points(x, teData["section"], pch=1, col="blue")
legend("bottomright", legend=c("prediction", "answer"), pch=c(4,1))
cor(nnPrediction2, teData["section"])
trData
head(trData)
lapply(NewsData, normalize)
lapply(NewsData, normalization)
lapply(NewsData[,-12], normalization)
scale(NewsData[,-12])
NewsDataN <- as.data.frame(lapply(NewsData[,-12], normalization))
NewsDataN$section <- NewsData[,12]
colnames(NewsDataN) <- colnames(NewsData)
str(NewsDataN)
summary(NewsDataN)
trData <- NewsDataN[idx,]
teData <- NewsDataN[-idx,]
nnFormula <- section ~ .
nnModel2 <- neuralnet(formula=nnFormula, data=trData, hidden=10)
Ei <- sample(c(1:Elen), 0.1*Elen); Ii <- sample(c(1:Ilen), 0.1*Ilen); Li <- sample(c(1:Llen), 0.1*Llen);
Pi <- sample(c(1:Plen), 0.1*Plen); Si <- sample(c(1:Slen), 0.1*Slen); Wi <- sample(c(1:Wlen), 0.1*Wlen);
idx <- c(
Ei,
Ii + Elen,
Li + Elen + Ilen,
Pi + Elen + Ilen + Llen,
Si + Elen + Ilen + Llen + Plen,
Wi + Elen + Ilen + Llen + Plen + Slen
)
NewsDataN <- as.data.frame(lapply(NewsData[,-12], normalization))
NewsDataN$section <- NewsData[,12]
colnames(NewsDataN) <- colnames(NewsData)
trData <- NewsDataN[idx,]
teData <- NewsDataN[-idx,]
nnFormula <- section ~ .
nnModel2 <- neuralnet(formula=nnFormula, data=trData, hidden=10)
nnModel2 <- neuralnet(formula=nnFormula, data=trData, hidden=5)
install.packages("openxlsx")
install.packages("openxlsx")
install.packages("openxlsx")
install.packages("devtools")
install.packages("nnet")
install.packages("neuralnet")
####################################################################################################
# Library
library(openxlsx)
library(dplyr)
library(e1071)
library(klaR)
library(kernlab)
####################################################################################################
# Variable
pathOne <- "D:/GitHub/tjproject/resources/"
sections <- c("econ", "IT", "life_cult", "politics", "soc", "world")
Csections <- c("Economy", "IT", "Life_Cult", "Politics", "Society", "World")
####################################################################################################
# Function
getData <- function(path, n) {
df <- data.frame()
for(i in c(1:n)) {
dfOne <- read.xlsx(path, sheet=i, colNames=T, rowNames=F)
df <- rbind(df, dfOne)
}
return(df)
}
getAllSectionData <- function(section) {
path1 <- paste(pathOne, section, "/2018_comment_data_", section, ".xlsx", sep="")
dfOne <- getData(path1, 2)
path2 <- paste(pathOne, section, "/2019_comment_data_", section, ".xlsx", sep="")
dfTwo <- getData(path2, 10)
path3 <- paste(pathOne, section, "/2018_view_data_", section, ".xlsx", sep="")
dfThree <- getData(path3, 2)
path4 <- paste(pathOne, section, "/2019_view_data_", section, ".xlsx", sep="")
dfFour <- getData(path4, 10)
df <- rbind(dfOne[-5], dfTwo[-5], dfThree[-5], dfFour[-5])
return(df)
}
getcolumns <- function(data, columns) {
len <- length(columns)
# cat(len, '\n')
if(len > 1) {
df <- data[columns[1]]
for(i in c(2:len)) {
df <- cbind(df, data[columns[i]])
}
} else { df <- data %>% select(columns) }
return(df)
}
getTAcc <- function(table) {
true <- table["TRUE"]
accuracy <- as.numeric(round(true/sum(table)*100, 2))
return(accuracy)
}
####################################################################################################
# Main
# Collect Data
Edf <- getAllSectionData(sections[1])
Idf <- getAllSectionData(sections[2])
Ldf <- getAllSectionData(sections[3])
Pdf <- getAllSectionData(sections[4])
Sdf <- getAllSectionData(sections[5])
Wdf <- getAllSectionData(sections[6])
# Get needed columns
reqCol <- c("currCmt","deleted","brokenPolicy","maleRatio","femaleRatio","X10","X20","X30","X40","X50","X60")
Edata <- getcolumns(Edf, reqCol)
Idata <- getcolumns(Idf, reqCol)
Ldata <- getcolumns(Ldf, reqCol)
Pdata <- getcolumns(Pdf, reqCol)
Sdata <- getcolumns(Sdf, reqCol)
Wdata <- getcolumns(Wdf, reqCol)
# Add its own section into a column
Edata$section <- "Economy"; Idata$section <- "IT"; Ldata$section <- "Life_Cult";
Pdata$section <- "Politics"; Sdata$section <- "Society"; Wdata$section <- "World";
# Collect into one massive column
NewsData <- rbind(Edata, Idata, Ldata, Pdata, Sdata, Wdata)
colnames(NewsData)
# Remove NAs
NewsData <- na.omit(NewsData)
# Change all Independent Variables to numeric
NewsData$currCmt <- as.numeric(NewsData$currCmt)
NewsData$deleted <- as.numeric(NewsData$deleted)
NewsData$brokenPolicy <- as.numeric(NewsData$brokenPolicy)
# Data Extract
Elen <- nrow(subset(NewsData, section == "Economy"))
Ilen <- nrow(subset(NewsData, section == "IT"))
Llen <- nrow(subset(NewsData, section == "Life_Cult"))
Plen <- nrow(subset(NewsData, section == "Politics"))
Slen <- nrow(subset(NewsData, section == "Society"))
Wlen <- nrow(subset(NewsData, section == "World"))
Ei <- sample(c(1:Elen), 0.8*Elen); Ii <- sample(c(1:Ilen), 0.8*Ilen); Li <- sample(c(1:Llen), 0.8*Llen);
Pi <- sample(c(1:Plen), 0.8*Plen); Si <- sample(c(1:Slen), 0.8*Slen); Wi <- sample(c(1:Wlen), 0.8*Wlen);
idx <- c(
Ei,
Ii + Elen,
Li + Elen + Ilen,
Pi + Elen + Ilen + Llen,
Si + Elen + Ilen + Llen + Plen,
Wi + Elen + Ilen + Llen + Plen + Slen
)
# Create Training and Testing Data
trData <- NewsData[ idx,]
teData <- NewsData[-idx,]
svmFormula <- section ~ .
##################################################
# 적절한 파라미터 찾기
param <- tune(svm, Species ~ ., data=iris, gamma=2^(-1:1), cost=2^(2:4))
attributes(param)
param$best.model
# rbfdot
rbfModel <- ksvm(svmFormula, data=trData, kernel="rbfdot")
rbfPred <- predict(rbfModel, teData)
rbfTable <- table(rbfPred, teData$section)
rbfTable
rbfResult <- rbfPred == teData$section
rbfResultT <- table(rbfResult)
rbfAcc <- getTAcc(rbfResultT)
rbfAcc
plot(rbfModel)
nrow(Edf)
nrow(NewsData)
