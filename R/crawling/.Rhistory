len <- length(vec)
if(len < 30) {
extra <- rep(NA, (30-len))
return(c(vec, extra))
} else {
return(vec)
}
}
check_url <- function(url) {
out <- tryCatch(
{
read_html(url)
}, error=function(cond) {
message("URL doesn't exist")
message(cond)
return(FALSE)
}, warning=function(cond) {
message("URL caused a warning")
message(cond)
return(FALSE)
},
)
return(out)
}
# Section - Politics, Economics, Society, Culture, World, Science, Photo, TV
################################################################################
sec_id <- as.character(c(100:105))
year <- c(2018)
month <- c('01','02','03','04','05','06','07','08','09','10','11','12')
day <- c(month, as.character(c(13:31)))
# 2019/11/05 Data
url_1 <- 'https://news.naver.com/main/ranking/popularDay.nhn?rankingType=popular_day'
url_2 <- '&sectionId='
url_3 <- '&date='
df <- data.frame()
for(y in year) {
for(m in month) {
for(d in day) {
# Disgard months with no 31st (except February)
if(m %in% c('04','06','09','11') & d == 31) { next }
# Disgard February (special cases)
if(m %in% c('02') & d >= 30) { next }
# Extract all category in this date
for(id in sec_id) {
# creates date variable
date <- paste(y, m, d, sep='')
print(date) # prints out the date each time to check
# crates complete url to crawl
url <- paste(url_1, url_2, id, url_3, date, sep='')
# checks if url is loadable. If not, it jumps to the next one
if(check_url(url) == FALSE) { next }
# loads the HTML code from the URL
html <- get_html(url)
item <- html %>% html_nodes('.ranking_item')
category <- html %>% html_nodes('.is_selected') %>% html_nodes('a') %>% html_text()
category <- category[2]
category <- rep(gsub('선택됨', '', category), 30)
title <- item %>% html_nodes('.ranking_headline') %>% html_nodes('a') %>% html_text()
source <- item %>% html_nodes('.ranking_office') %>% html_text()
view <- item %>% html_nodes('.ranking_view') %>% html_text()
# if(length(view) == 0) { view <- rep(NA, 30) }
title <- check_cat(title)
source <- check_cat(source)
view <- check_cat(view)
date <- rep(date, 30)
temp_df <- data.frame(category, title, source, view, date)
df <- rbind(df, temp_df)
}
}
}
}
write.csv(df, 'news.csv')
################################################################################
# Naver News
################################################################################
# Library imports
library(rvest)
# Functions
get_html <- function(url) {
html <- read_html(url)
return(html)
}
check_cat <- function(vec) {
len <- length(vec)
if(len < 30) {
extra <- rep(NA, (30-len))
return(c(vec, extra))
} else {
return(vec)
}
}
check_url <- function(url) {
out <- tryCatch(
{
read_html(url)
}, error=function(cond) {
message("URL doesn't exist")
message(cond)
return(FALSE)
}, warning=function(cond) {
message("URL caused a warning")
message(cond)
return(FALSE)
}, finally={
message("try and catch end")
}
)
return(out)
}
# Section - Politics, Economics, Society, Culture, World, Science, Photo, TV
################################################################################
sec_id <- as.character(c(100:105))
year <- c(2018)
month <- c('01','02','03','04','05','06','07','08','09','10','11','12')
day <- c(month, as.character(c(13:31)))
# 2019/11/05 Data
url_1 <- 'https://news.naver.com/main/ranking/popularDay.nhn?rankingType=popular_day'
url_2 <- '&sectionId='
url_3 <- '&date='
df <- data.frame()
for(y in year) {
for(m in month) {
for(d in day) {
# Disgard months with no 31st (except February)
if(m %in% c('04','06','09','11') & d == 31) { next }
# Disgard February (special cases)
if(m %in% c('02') & d >= 30) { next }
# Extract all category in this date
for(id in sec_id) {
# creates date variable
date <- paste(y, m, d, sep='')
print(date) # prints out the date each time to check
# crates complete url to crawl
url <- paste(url_1, url_2, id, url_3, date, sep='')
# checks if url is loadable. If not, it jumps to the next one
if(check_url(url) == FALSE) { next }
# loads the HTML code from the URL
html <- get_html(url)
item <- html %>% html_nodes('.ranking_item')
category <- html %>% html_nodes('.is_selected') %>% html_nodes('a') %>% html_text()
category <- category[2]
category <- rep(gsub('선택됨', '', category), 30)
title <- item %>% html_nodes('.ranking_headline') %>% html_nodes('a') %>% html_text()
source <- item %>% html_nodes('.ranking_office') %>% html_text()
view <- item %>% html_nodes('.ranking_view') %>% html_text()
# if(length(view) == 0) { view <- rep(NA, 30) }
title <- check_cat(title)
source <- check_cat(source)
view <- check_cat(view)
date <- rep(date, 30)
temp_df <- data.frame(category, title, source, view, date)
df <- rbind(df, temp_df)
}
}
}
}
write.csv(df, 'news.csv')
message(T)
################################################################################
# Naver News
################################################################################
# Library imports
library(rvest)
# Functions
get_html <- function(url) {
html <- read_html(url)
return(html)
}
check_cat <- function(vec) {
len <- length(vec)
if(len < 30) {
extra <- rep(NA, (30-len))
return(c(vec, extra))
} else {
return(vec)
}
}
check_url <- function(url) {
out <- tryCatch(
{
read_html(url)
message(T)
}, error=function(cond) {
message("URL doesn't exist")
message(cond)
return(FALSE)
}, warning=function(cond) {
message("URL caused a warning")
message(cond)
return(FALSE)
}, finally={
message("try and catch end")
}
)
return(out)
}
# Section - Politics, Economics, Society, Culture, World, Science, Photo, TV
################################################################################
sec_id <- as.character(c(100:105))
year <- c(2018)
month <- c('01','02','03','04','05','06','07','08','09','10','11','12')
day <- c(month, as.character(c(13:31)))
# 2019/11/05 Data
url_1 <- 'https://news.naver.com/main/ranking/popularDay.nhn?rankingType=popular_day'
url_2 <- '&sectionId='
url_3 <- '&date='
df <- data.frame()
for(y in year) {
for(m in month) {
for(d in day) {
# Disgard months with no 31st (except February)
if(m %in% c('04','06','09','11') & d == 31) { next }
# Disgard February (special cases)
if(m %in% c('02') & d >= 30) { next }
# Extract all category in this date
for(id in sec_id) {
# creates date variable
date <- paste(y, m, d, sep='')
print(date) # prints out the date each time to check
# crates complete url to crawl
url <- paste(url_1, url_2, id, url_3, date, sep='')
# checks if url is loadable. If not, it jumps to the next one
if(check_url(url) == FALSE) { next }
# loads the HTML code from the URL
html <- get_html(url)
item <- html %>% html_nodes('.ranking_item')
category <- html %>% html_nodes('.is_selected') %>% html_nodes('a') %>% html_text()
category <- category[2]
category <- rep(gsub('선택됨', '', category), 30)
title <- item %>% html_nodes('.ranking_headline') %>% html_nodes('a') %>% html_text()
source <- item %>% html_nodes('.ranking_office') %>% html_text()
view <- item %>% html_nodes('.ranking_view') %>% html_text()
# if(length(view) == 0) { view <- rep(NA, 30) }
title <- check_cat(title)
source <- check_cat(source)
view <- check_cat(view)
date <- rep(date, 30)
temp_df <- data.frame(category, title, source, view, date)
df <- rbind(df, temp_df)
}
}
}
}
write.csv(df, 'news.csv')
url
check_url(url)
result <- check_url(url)
result
################################################################################
# Naver News
################################################################################
# Library imports
library(rvest)
# Functions
get_html <- function(url) {
html <- read_html(url)
return(html)
}
check_cat <- function(vec) {
len <- length(vec)
if(len < 30) {
extra <- rep(NA, (30-len))
return(c(vec, extra))
} else {
return(vec)
}
}
check_url <- function(url) {
out <- tryCatch(
{
read_html(url)
message(T)
return(T)
}, error=function(cond) {
message("URL doesn't exist")
message(cond)
return(FALSE)
}, warning=function(cond) {
message("URL caused a warning")
message(cond)
return(FALSE)
}, finally={
message("try and catch end")
}
)
return(out)
}
# Section - Politics, Economics, Society, Culture, World, Science, Photo, TV
################################################################################
sec_id <- as.character(c(100:105))
year <- c(2018)
month <- c('01','02','03','04','05','06','07','08','09','10','11','12')
day <- c(month, as.character(c(13:31)))
# 2019/11/05 Data
url_1 <- 'https://news.naver.com/main/ranking/popularDay.nhn?rankingType=popular_day'
url_2 <- '&sectionId='
url_3 <- '&date='
df <- data.frame()
for(y in year) {
for(m in month) {
for(d in day) {
# Disgard months with no 31st (except February)
if(m %in% c('04','06','09','11') & d == 31) { next }
# Disgard February (special cases)
if(m %in% c('02') & d >= 30) { next }
# Extract all category in this date
for(id in sec_id) {
# creates date variable
date <- paste(y, m, d, sep='')
print(date) # prints out the date each time to check
# crates complete url to crawl
url <- paste(url_1, url_2, id, url_3, date, sep='')
# checks if url is loadable. If not, it jumps to the next one
if(check_url(url) == FALSE) { next }
# loads the HTML code from the URL
html <- get_html(url)
item <- html %>% html_nodes('.ranking_item')
category <- html %>% html_nodes('.is_selected') %>% html_nodes('a') %>% html_text()
category <- category[2]
category <- rep(gsub('선택됨', '', category), 30)
title <- item %>% html_nodes('.ranking_headline') %>% html_nodes('a') %>% html_text()
source <- item %>% html_nodes('.ranking_office') %>% html_text()
view <- item %>% html_nodes('.ranking_view') %>% html_text()
# if(length(view) == 0) { view <- rep(NA, 30) }
title <- check_cat(title)
source <- check_cat(source)
view <- check_cat(view)
date <- rep(date, 30)
temp_df <- data.frame(category, title, source, view, date)
df <- rbind(df, temp_df)
}
}
}
}
write.csv(df, 'news.csv')
################################################################################
# Naver News
################################################################################
# Library imports
library(rvest)
# Functions
get_html <- function(url) {
html <- read_html(url)
return(html)
}
check_cat <- function(vec) {
len <- length(vec)
if(len < 30) {
extra <- rep(NA, (30-len))
return(c(vec, extra))
} else {
return(vec)
}
}
check_url <- function(url) {
out <- tryCatch(
{
read_html(url)
return(T)
}, error=function(cond) {
message("URL doesn't exist")
message(cond)
return(FALSE)
}, warning=function(cond) {
message("URL caused a warning")
message(cond)
return(FALSE)
}, finally={}
)
return(out)
}
# Section - Politics, Economics, Society, Culture, World, Science, Photo, TV
################################################################################
sec_id <- as.character(c(100:105))
year <- c(2018)
month <- c('01','02','03','04','05','06','07','08','09','10','11','12')
day <- c(month, as.character(c(13:31)))
# 2019/11/05 Data
url_1 <- 'https://news.naver.com/main/ranking/popularDay.nhn?rankingType=popular_day'
url_2 <- '&sectionId='
url_3 <- '&date='
df <- data.frame()
for(y in year) {
for(m in month) {
for(d in day) {
# Disgard months with no 31st (except February)
if(m %in% c('04','06','09','11') & d == 31) { next }
# Disgard February (special cases)
if(m %in% c('02') & d >= 30) { next }
# Extract all category in this date
for(id in sec_id) {
# creates date variable
date <- paste(y, m, d, sep='')
print(date) # prints out the date each time to check
# crates complete url to crawl
url <- paste(url_1, url_2, id, url_3, date, sep='')
# checks if url is loadable. If not, it jumps to the next one
if(check_url(url) == FALSE) { next }
# loads the HTML code from the URL
html <- get_html(url)
item <- html %>% html_nodes('.ranking_item')
category <- html %>% html_nodes('.is_selected') %>% html_nodes('a') %>% html_text()
category <- category[2]
category <- rep(gsub('선택됨', '', category), 30)
title <- item %>% html_nodes('.ranking_headline') %>% html_nodes('a') %>% html_text()
source <- item %>% html_nodes('.ranking_office') %>% html_text()
view <- item %>% html_nodes('.ranking_view') %>% html_text()
# if(length(view) == 0) { view <- rep(NA, 30) }
title <- check_cat(title)
source <- check_cat(source)
view <- check_cat(view)
date <- rep(date, 30)
temp_df <- data.frame(category, title, source, view, date)
df <- rbind(df, temp_df)
}
}
}
}
write.csv(df, 'news.csv')
setwd("D:/GitHub/tjproject/R/crawling")
################################################################################
# Initial coding
url <- "https://news.naver.com/main/read.nhn?m_view=1&mode=LSD&mid=shm&sid1=100&oid=022&aid=0003412842"
html <- read_html(url)
################################################################################
# Crawling the comments from politics news.
################################################################################
# Library imports
library(rvest)
# Functions
cleans <- function(x) {
library(stringr)
x <- gsub("\t", "", x)
x <- gsub("\r", "", x)
x <- gsub("\n", "", x)
x <- str_trim(x)
return(x)
}
# Initialisation
setwd("D:/GitHub/tjproject/R/crawling")
url_list <- c(
"https://news.naver.com/main/read.nhn?m_view=1&mode=LSD&mid=shm&sid1=100&oid=022&aid=0003412842"
)
df <- data.frame()
################################################################################
# Initial coding
url <- "https://news.naver.com/main/read.nhn?m_view=1&mode=LSD&mid=shm&sid1=100&oid=022&aid=0003412842"
html <- read_html(url)
content <- html %>% html_nodes(".content") # Top content
title <- content %>% html_nodes(".article_header") %>% html_nodes(".article_info") %>% html_nodes("h3") %>% html_nodes("a") %>% html_text() # title of the news
date <- content %>% html_nodes(".article_header") %>% html_nodes(".article_info") %>% html_nodes(".sponsor") %>% html_nodes("span") %>% html_text() # Date of the news written
cbox <- content %>% html_nodes(".u_cbox_area")
id <- cbox %>% html_nodes(".u_cobx_info") %>% html_nodes(".u_cbox_nick") %>% html_text()
comment <- cbox %>% html_nodes(".u_cbox_text_wrap") %>% html_nodes("span") %>% html_text()
info <- cbox %>% html_nodes(".u_cbox_info_base") %>% html_nodes(".u_cobx_date") %>% html_text()
id
comment
info
html
install.packages('discord')
library(discord)
content
content %>% html_nodes('.article_header')
content %>% html_nodes('.article_header') %>% html_nodes('.article_info')
content %>% html_nodes('.article_header') %>% html_nodes('.article_info') %>% html_nodes(h3)
content %>% html_nodes('.article_header') %>% html_nodes('.article_info') %>% html_nodes(a)
content %>% html_nodes('.article_header') %>% html_nodes('.article_info') %>% html_nodes('h3')
content %>% html_nodes('.article_header') %>% html_nodes('.article_info') %>% html_nodes('h3') %>% html_nodes('a')
content %>% html_nodes('.article_header') %>% html_nodes('.article_info') %>% html_nodes('h3') %>% html_nodes('a') %>% html_text()
title <- content %>% html_nodes(".article_header") %>% html_nodes(".article_info") %>% html_nodes("h3") %>% html_nodes("a") %>% html_text() # title of the news
title
id
comment
date
date <- date[1]
date\
date
cbox <- content %>% html_nodes(".u_cbox_area")
cbox
cbox <- html %>% html_nodes(".u_cbox_area")
cbox
cbox <- html %>% html_nodes(".u_cbox")
cbox
cbox <- html %>% html_nodes(".u_cbox")
cobx
cbox
content
content %>% html_nodes('.u_cbox')
content %>% html_nodes('.u_cbox_wrap')
content[1] %>% html_nodes('.u_cbox')
content[1]
content[1] %>% html_nodes('u_cbox')
content[1] %>% html_nodes('.u_cbox')
content %>% html_nodes('.u_cbox')
content %>% html_nodes('.u_cbox_list')
html %>% html_nodes('.u_cbox_list')
content
content %>% html_text()
content %>% html_nodes('div')
content %>% html_nodes('div') %>% html_nodes('.u_cbox')
content %>% html_nodes('div') %>% html_nodes('.content')
content %>% html_nodes('.content')
content %>% html_nodes('.content') %>% html('.u_cobx')
content %>% html_nodes('.content') %>% html('.u_cbox')
content %>% html_nodes('.content') %>% html_nodes('.u_cbox')
html %>% html_nodes('.container') %>% html_nodes('tbody') %>% html_nodes('tr') %>% html_nodes('.content') %>% html_nodes('.u_cbox')
html %>% html_nodes('.container') %>% html_nodes('tbody') %>% html_nodes('tr') %>% html_nodes('.content')
html %>% html_nodes('.container') %>% html_nodes('tbody') %>% html_nodes('tr')
html %>% html_nodes('.container') %>% html_nodes('tbody')
html %>% html_nodes('.container')
html %>% html_nodes('.content')
html %>% html_nodes('.content') %>% html_nodes('li')
